{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Tutorial\n",
    "**In this tutorial, you are going to estimate a linear filter using gradient descent.**\n",
    "\n",
    "There are many gradient descent routines both in matlab and in python.  The goal of this tutorial is to:\n",
    "\n",
    "1. Realize how simple gradient descent can be\n",
    "2. Obtain a visual schema on how it works on an error surface\n",
    "3. Understand its relationship to regularization techniques\n",
    "\n",
    "To do this, we'll simulate some data, and an artificial response to that data. Then we'll fit several models that attempt to uncover the relationship between the input / response data we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a toy model: a 2d stimulus (2 pixels) and a simple filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a two dimensional stimulus (e.g two pixels) with correlations and 100 samples (e.g. points in time)\n",
    "# First pixel data\n",
    "nsamp = 100\n",
    "x1 = np.random.randn(1, nsamp)\n",
    "\n",
    "# Second pixel that is correlated with the first\n",
    "x2 = .4 * x1 + .6 * np.random.randn(1, nsamp)\n",
    "\n",
    "# Concatinate into a stimulus matrix - here rows are dimensions and columns are time points.\n",
    "x = np.vstack([x1, x2])\n",
    "\n",
    "## Generate a filter and the corresponding one dimensional response \n",
    "# Set weights on each channel\n",
    "h = np.array([5, 7]).reshape([2, 1])\n",
    "\n",
    "# Make response of system - this is the output of our toy neuron\n",
    "y = np.dot(x.T, h)\n",
    "\n",
    "# Plot it as a time series\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Response')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Error Surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to pretend we don't know h and make a search for h values by settting up \n",
    "# a range of potential values for h1 and h2\n",
    "\n",
    "h1, h2 = np.meshgrid(np.arange(-1, 10, .2), np.arange(-1, 10, .2))\n",
    "hs = np.vstack([h1.ravel(), h2.ravel()])\n",
    "\n",
    "# get responses from each set of weights\n",
    "ys = np.dot(x.T, hs)\n",
    "\n",
    "# calculate error between the response, y, and each of the possible responses, ys.  \n",
    "err = np.sum((y - ys) ** 2, 0)\n",
    "\n",
    "# reshape for plotting\n",
    "err = err.reshape(h1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot contour of error surface. Note the shape of the surface is angled\n",
    "# because the two variable are correlated.\n",
    "fig, ax = plt.subplots()\n",
    "ax.contour(h1, h2, err, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. The gradient descent solution.\n",
    "* Plot the actual solution as a large cross on the contour plot\n",
    "* Print the value of the error surface at this solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Gradient descent with more noise.\n",
    " Next we'll decrease the SNR of our response and see how this changes the solution found by the model.\n",
    " \n",
    "* Generate a new response (call it `yr`, for \"y real\") that includes noise (SNR ~1). \n",
    "* Repeat some of the code above to obtain a new error surface as you would with real data.\n",
    "* Plot this on a new figure and compare to the figure above.\n",
    "* Calculate the minimum of this error and compare to the minimum of the noise free error surface.\n",
    "* What happens to the error surface if you only have 10 data points (and noise)? \n",
    "* Calculate the surface / solution for this condition plot it in on a third plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the contour / solution that you found:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now with fewer data points \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the error surfaces do not become noisy but that the minimum moves either\n",
    "when we add noise or when we reduce the number of points.\n",
    "The minimum value in the noisy situation is also a postive, non-zero number.  This minimum is \n",
    "not found at the actual values of the linear filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3. The analytical solution\n",
    "Next we'll solve for the MSE solution using an analytical solution that doesn't depend on gradient descent.\n",
    "\n",
    "* Solve for the Least MSE solution using the analytical solution (calculate the cross and auto correlation and take ratio).\n",
    "> Note: in numpy: matrix multipy is `np.dot()` and matrix inverse is `np.linalg.inv()`\n",
    "* Plot this solution as well as the correct solutions on your contour plots of figures 2 and 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solve for the analytic solution to model weights\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot solution on contour\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for the subset of data points\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot solution on contour\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4. Comparing gradient descent with the MSE solution.\n",
    "Now we're going to solve using gradient descent to show that you get the same answer as the MSE solution.\n",
    "\n",
    "# XXX Frederic: these instructions are a little bit confusing to me. I'm not sure what you mean by \"use the analytical solution for the gradient.\n",
    "You are going to solve it using the x and yr data. Use the\n",
    "analytical solution for the gradient.  Fill in the missing code and\n",
    "print out the final solution. Stop the descent when error is less than\n",
    "the noise power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set a step size and a fixed number of maximum steps\n",
    "nsteps = 500   # This is a maximum number of steps\n",
    "vary = np.var(yr)\n",
    "hscale = 10    # This is a guess on the variance of the h parameters\n",
    "stepsize = hscale / vary # This is to get stepsizes with the correct units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frederic: Add `SOLUTION BEGIN` and `SOLUTION END` with two comment marks (`##`) around any of the code sections below that you want taken out...I assume you'd like to keep the bulk of the `for` loop in place for students! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize hhat at origin and allocate space\n",
    "hhat = np.zeros([2, nsteps + 1])  #  We will be keeeping track of hhat during our descent\n",
    "\n",
    "# loop for a certain number of iterations\n",
    "# and mark when your reach the noise level...\n",
    "# NOTE we are always going to the end of the loop for display purposes\n",
    "\n",
    "totstep = -1\n",
    "for ii in range(nsteps):\n",
    "    \n",
    "    # calculate the gradient at hhat\n",
    "    preds = np.dot(x.T, hhat[:,ii])\n",
    "    err = preds - yr.squeeze()\n",
    "    grad = np.dot(x, err) / nsamp\n",
    "    \n",
    "    # update hhat using stepsize and gradient\n",
    "    hhat[:, ii + 1] = hhat[:, ii] - stepsize * grad\n",
    "    \n",
    "    # calculate mean square error\n",
    "    mserr = np.mean((np.dot(x.T, hhat[:, ii+1]) - yr.squeeze()) ** 2)\n",
    "    \n",
    "    # set stopping condition when error is below noise power.\n",
    "    if mserr < (vary / (SNR + 1)):   # Find the first time the error is below the estimate of the noise\n",
    "        if totstep == -1:\n",
    "            totstep = ii+1\n",
    "\n",
    "if totstep == -1:    \n",
    "    totstep = ii+1\n",
    "\n",
    "msg = 'Gradient solution of h = [{:.2f}, {:.2f}] reached after {} steps\\n'.format(hhat[0, totstep], hhat[1, totstep], totstep)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5. Plotting the gradient descent path.\n",
    "Next we'll plot the gradient descent path on our contour plots in of figure 2. This will show us how the model converged on its solution.\n",
    "\n",
    "* Plot the gradient descent path on the countour plot from figure 2\n",
    "* Comment on why early stopping could give you a more general solution.  \n",
    "\n",
    "XXX Not sure what you mean by \"general\" solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6. Comparing gradient descent and MSE with fewer datapoints\n",
    "XXX - I think we should refer to specific questions instead of figure numbers. OR, we should include \"figure N\" in the comments or titles of figures so that people know what we're referring to.\n",
    "\n",
    "Repeat the calculations of Problem 4 and 5 with figure 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize hhatsmall at origin and allocate space\n",
    "hhatsmall = np.zeros([2, nsteps + 1])  #  We will be keeeping track of hhatsmall during our descent\n",
    "varysmall = np.var(yrsmall)\n",
    "\n",
    "# loop for a certain number of iterations\n",
    "# and mark when your reach the noise level...\n",
    "totstepsmall = -1\n",
    "for ii in range(nsteps):\n",
    "    \n",
    "    # calculate the gradient at hhatsmall\n",
    "    preds = np.dot(xsmall.T, hhatsmall[:,ii])\n",
    "    err = preds - yrsmall.squeeze()\n",
    "    grad = np.dot(xsmall, err) / nsamp\n",
    "    \n",
    "    # update hhatsmall using stepsize and gradient\n",
    "    hhatsmall[:, ii + 1] = hhatsmall[:, ii] - stepsize * grad\n",
    "    \n",
    "    # calculate error\n",
    "    mserr = np.mean((np.dot(xsmall.T, hhatsmall[:, ii+1]) - yrsmall.squeeze()) ** 2)\n",
    "    \n",
    "    # set stopping condition when error is below noise power.\n",
    "    if mserr < (varysmall / (SNR + 1)):\n",
    "        if totstepsmall == -1:\n",
    "            totstepsmall = ii + 1\n",
    "    \n",
    "\n",
    "if totstepsmall == -1:\n",
    "    totstepsmall = ii + 1\n",
    "\n",
    "msg = 'Gradient solution of h = [{:.2f}, {:.2f}] reached after {} steps\\n'.format(hhatsmall[0, totstepsmall], hhatsmall[1, totstepsmall], totstepsmall)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the descent path on the contour surface\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6. Solve using coordinate descent\n",
    " Next we'll use a different method for gradient descent, called \"coordinate descent\".\n",
    " \n",
    " Fill the code to solve the problem using coordinate descent and plot the \n",
    " trajectories on figure 2 and figure 3 the path with a different\n",
    " color line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 500\n",
    "\n",
    "# initialize hhat at origin and allocate space\n",
    "hhatcd = np.zeros([2, nsteps + 1])  #  We will be keeeping track of hhat during our descent\n",
    "\n",
    "# loop for a certain number of iterations\n",
    "# and mark when your reach the noise level...\n",
    "totstepcd = -1\n",
    "for ii in range(nsteps):\n",
    "    \n",
    "    # calculate the gradient at hhat\n",
    "    preds = np.dot(x.T, hhatcd[:,ii])\n",
    "    err = preds - yr.squeeze()\n",
    "    grad = np.dot(x, err) / nsamp\n",
    "    \n",
    "    # update hhat using stepsize and gradient\n",
    "    ix_maxgrad = np.argmax(np.abs(grad))\n",
    "    maxgrad = grad[ix_maxgrad]\n",
    "    hhatcd[:, ii+1] = hhatcd[:, ii]   # Copy previous\n",
    "    hhatcd[ix_maxgrad, ii + 1] = hhatcd[ix_maxgrad, ii] - stepsize * maxgrad   # Change along a single coordinate\n",
    "    \n",
    "    # calculate error\n",
    "    mserr = np.mean((np.dot(x.T, hhatcd[:, ii+1]) - yr.squeeze()) ** 2)\n",
    "    \n",
    "    # set stopping condition when error is below noise power.\n",
    "    if mserr < (vary / (SNR + 1)):\n",
    "        if totstepcd == -1:\n",
    "            totstepcd = ii + 1\n",
    "    \n",
    "\n",
    "if totstepcd == -1:\n",
    "    totstepcd = ii + 1\n",
    "\n",
    "msg = 'Coordinate descent solution of h = [{:.2f}, {:.2f}] reached after {} steps\\n'.format(\n",
    "    hhatcd[0, totstepcd], hhatcd[1, totstepcd], totstepcd)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the path once again\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat for the smaller data shown in figure 3. \n",
    "nsteps = 500\n",
    "\n",
    "# initialize hhat at origin and allocate space\n",
    "hhatcdsmall = np.zeros([2, nsteps + 1])  #  We will be keeeping track of hhat during our descent\n",
    "\n",
    "# loop for a certain number of iterations\n",
    "# and mark when your reach the noise level...\n",
    "totstepcdsmall = -1\n",
    "for ii in range(nsteps):\n",
    "    \n",
    "    # calculate the gradient at hhat\n",
    "    preds = np.dot(xsmall.T, hhatcdsmall[:,ii])\n",
    "    err = preds - yrsmall.squeeze()\n",
    "    grad = np.dot(xsmall, err) / nsamp\n",
    "    \n",
    "    # update hhat using stepsize and gradient\n",
    "    ix_maxgrad = np.argmax(np.abs(grad))\n",
    "    maxgrad = grad[ix_maxgrad]\n",
    "    hhatcdsmall[:, ii+1] = hhatcdsmall[:, ii]\n",
    "    hhatcdsmall[ix_maxgrad, ii + 1] = hhatcdsmall[ix_maxgrad, ii] - stepsize * maxgrad\n",
    "    \n",
    "    # calculate error\n",
    "    mserr = np.mean((np.dot(xsmall.T, hhatcdsmall[:, ii+1]) - yrsmall.squeeze()) ** 2)\n",
    "    \n",
    "    # set stopping condition when error is below noise power.\n",
    "    if mserr < (varysmall / (SNR + 1)):\n",
    "        if totstepcdsmall == -1:\n",
    "            totstepcdsmall = ii + 1\n",
    "    \n",
    "\n",
    "if totstepcdsmall == -1:\n",
    "    totstepcdsmall = ii + 1\n",
    "\n",
    "msg = 'Coordinate descent solution of hsmall = [{:.2f}, {:.2f}] reached after {} steps\\n'.format(\n",
    "    hhatcdsmall[0, totstepcdsmall], hhatcdsmall[1, totstepcdsmall], totstepcdsmall)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the path once again\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7. Regularizing with Ridge regression\n",
    "\n",
    " On figures 2 and 3, show the path corresponds to ridge regression.\n",
    " Remember that you calculated the auto and cross correlation in exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The auto and cross correlation were obtained in Problem 3 with\n",
    "# cross_xy = np.dot(x, yr) / nsamp\n",
    "# auto_xx = np.dot(x, x.T) / nsamp\n",
    "# Eigenvalue decomposition of auto_xx using the svd\n",
    "u, s, v = np.linalg.svd(auto_xx)\n",
    "\n",
    "# hyper parameter lambda as fraction of largest eigenvalue\n",
    "alphas = [10, 1, 0.5, 0.1, 0.05, 0.01, .005, .001, .0005, .0001, .00001, 0]\n",
    "nalpha = len(alphas)\n",
    "\n",
    "maxs = np.max(s)\n",
    "hr = np.zeros([2, nalpha])\n",
    "\n",
    "totstepridge = -1\n",
    "for ii in range(nalpha):\n",
    "    # Calcuate inverse for each ridge parameter\n",
    "    XXinv = np.dot(v.T, np.dot(np.diag(1. / (s + (maxs * alphas[ii]))), u))\n",
    "    \n",
    "    # Estimate parameters for each ridge value\n",
    "    hr[:, ii] = np.dot(XXinv, cross_xy).squeeze()\n",
    "    \n",
    "    # calculate error\n",
    "    mserr = np.mean( (np.dot(x.T, hr[:, ii]) - yr.squeeze()) ** 2)\n",
    "    # set stopping criteria\n",
    "    if mserr < (vary / (SNR + 1)):\n",
    "        if totstepridge == -1:\n",
    "            totstepridge = ii\n",
    "\n",
    "            \n",
    "if totstepridge == -1:\n",
    "    totstepridge = ii\n",
    "    \n",
    "msg = 'Ridge solution of h = [{:.2f}, {:.2f}] reached for alpha {:g}'.format(\n",
    "    hr[0, totstepridge], hr[1, totstepridge], alphas[totstepridge])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the path for the ridge solutions\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The auto and cross correlation were obtained in Problem 3 with\n",
    "# cross_xy = np.dot(x, yr) / nsamp\n",
    "# auto_xx = np.dot(x, x.T) / nsamp\n",
    "# Eigenvalue decomposition of auto_xx using the svd\n",
    "u, s, v = np.linalg.svd(auto_xx_small)\n",
    "\n",
    "# hyper parameter lambda as fraction of largest eigenvalue\n",
    "alphas = [10, 1, 0.5, 0.1, 0.05, 0.01, .005, .001, .0005, .0001, .00001, 0]\n",
    "nalpha = len(alphas)\n",
    "\n",
    "maxs = np.max(s)\n",
    "hrsmall = np.zeros([2, nalpha])\n",
    "\n",
    "totstepridgesmall = -1\n",
    "for ii in range(nalpha):\n",
    "    # Calcuate inverse for each ridge parameter\n",
    "    XXinv = np.dot(v.T, np.dot(np.diag(1. / (s + (maxs * alphas[ii]))), u))\n",
    "    \n",
    "    # Estimate parameters for each ridge value\n",
    "    hrsmall[:, ii] = np.dot(XXinv, cross_xy_small).squeeze()\n",
    "    \n",
    "    # calculate error\n",
    "    mserr = np.mean( (np.dot(xsmall.T, hrsmall[:, ii]) - yrsmall.squeeze()) ** 2)\n",
    "    \n",
    "    # set stopping criteria\n",
    "    if mserr < varysmall / (SNR + 1):\n",
    "        if totstepridgesmall == -1:\n",
    "            totstepridgesmall = ii\n",
    "\n",
    "            \n",
    "if totstepridgesmall == -1:\n",
    "    totstepridgesmall = ii\n",
    "    \n",
    "msg = 'Ridge solution of h = [{:.2f}, {:.2f}] reached for alpha {:g}'.format(\n",
    "    hrsmall[0, totstepridgesmall], hrsmall[1, totstepridgesmall], alphas[totstepridgesmall])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot ridge path on contour\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
